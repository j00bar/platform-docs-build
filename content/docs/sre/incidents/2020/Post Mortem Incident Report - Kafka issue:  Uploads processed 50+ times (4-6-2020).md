---
date: 2020-04-09 18:27:52.578000
title: 'Post Mortem Incident Report - Kafka issue:  Uploads processed 50+ times (4-6-2020)'
---
## Overview

Uploads appeared to be processed very slowly and many uploads were being
processed 50 times in our apps. This caused a large backlog of uploads
to form.

## What Happened

We received several “insights-prod (UploadElapsedTimesTooLongHard
pagerduty mnm)” alerts during the weekend of 4/4. On Monday, a group was
formed to troubleshoot the issue which surfaced that uploads were not
being processed correctly and a large backlog was forming.

Between Monday and Tuesday, a group of engineers embarked on several
troubleshooting paths. The situation was initially presented as a Kafka
issue, which caused significant troubleshooting effort to be focused on
performance challenges.

A change made on Thursday, April 2 to increase the Kafka message size
was identified as a potential complication. This change was rolled back
on the evening of April 6. On the morning of April 7, it was determined
that the message size change roll back did not resolve the issue.

Insights-puptoo was released on Friday that included a newer version of
the kafka-python library.

Jjaggars-hypothesis:

Kafka-python update means that insights-puptoo stopped committing
offsets. Eventually kafka would serve the partition again on the next
call to poll(). Downstream, every consumer got many many duplicates
generated by puptoo. The additional load caused some clients to fail
which triggered CG rebalances which meant that progress wasn’t made
downstream either.

## Contributing Factors

*Include a description of any conditions that contributed to the issue.
If there were any actions taken that exacerbated the issue, also include
them here with the intention of learning from any mistakes made during
the resolution process.*

## Resolution

*Puptoo was modified to use the* confluent\_kafka *library.*

## Impact

*Be specific here. Include numbers such as customers affected, cost to
business, etc.*

<table>
<tbody>
<tr class="odd">
<td>Upload processing throughput was severely degraded</td>
<td>4 days (Friday, April 3 - Tuesday, April 7)</td>
</tr>
</tbody>
</table>

## Timeline

*Some important times to include: (1) time the contributing factor
began, (2) time of the page, (3) time that the status page was updated
(i.e. when the incident became public), (4) time of any significant
actions, (5) time the SEV-2/1 ended, (6) links to tools/logs that show
how the timestamp was arrived at.*

<table>
<thead>
<tr class="header">
<th><strong>Time (EST)</strong></th>
<th><strong>Notes</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>April 4, 5:58am</td>
<td>First <em>insights-prod (UploadElapsedTimesTooLongHard pagerduty mnm)</em> alert fires. It self-resolves in 10 minutes. This repeats several times over the course of the day</td>
</tr>
<tr class="even">
<td>April 4, 7:26pm</td>
<td>Cmoore silences the alert, plans to schedule call on Monday morning</td>
</tr>
<tr class="odd">
<td>April 6, 10am</td>
<td>Group forms on Hangout to discuss the weekend alerts. It is determined that there is a deeper issue, but the problem still presents as a Kafka performance issue. Group agrees to continue troubleshooting and meet at 3:30pm</td>
</tr>
<tr class="even">
<td>April 6, 3:30pm</td>
<td>Group reconvenes to continue troubleshooting. This leads to several avenues being explored over the next 2.5 hours.</td>
</tr>
<tr class="odd">
<td>April 6, 5:32pm</td>
<td>Decide to roll back Kafka message size change that was deployed on April 3. Reach out to shaund to understand impact to xjoin. Shaund pulls in chrisk.</td>
</tr>
<tr class="even">
<td>April 6, 6:01pm</td>
<td>Begin rolling back Kafka message size change</td>
</tr>
<tr class="odd">
<td>April 6, 6:20pm</td>
<td>Roll back complete, waiting for system to stabilize, agree to transition to Slack <a href="https://ansible.slack.com/archives/CDVFMQU31/p1586211741092100"><span class="underline">thread</span></a> and revisit later in the evening</td>
</tr>
<tr class="even">
<td>April 7, 1:34pm</td>
<td>Update puptoo to use confluent_kafka library</td>
</tr>
<tr class="odd">
<td>April 7, 3:00pm</td>
<td>sadams reports in Slack <a href="https://ansible.slack.com/archives/CDVFMQU31/p1586211741092100"><span class="underline">thread</span></a> that messages are being processed again</td>
</tr>
</tbody>
</table>

## How’d We Do?

### What went well?

*List anything you did well and want to call out. It's OK to not list
anything.*

  - > Cross workstream collaboration worked well. Several engineers
    > across many teams gathered at 10am, broke up and did additional
    > research, gathered at 3:30 to reconvene

  - > Efficient communication was handled by the troubleshooting team by
    > Hangout and Slack

### What didn’t go so well?

*List anything you think we didn't do very well. The intent is that we
should follow up on all points here to improve our processes.*

  - > The person on-call (Asa) was alarmed repeatedly, but should have
    > been empowered to mute the alarm until regular working hours,
    > since we didn’t escalate the issue over the weekend.

  - > The alerting in place was very general, did not provide enough
    > detail to identify the issue at alert time

  - > Be more skeptical of systemic problems. Is the problem observed
    > the previous Friday indicative of a larger problem?

  - > Alert was not specific enough to consistently be actionable
    
      - > Previous occurrences have either self-resolved or involved a
        > specific action to fix
    
      - > Alert should be specific and actionable

  - 
## Action Items

*Each action item should be in the form of a JIRA ticket, and each
ticket should have the same set of two tags: “sev1\_YYYYMMDD” (such as
sev1\_20150911) and simply “sev1”. Include action items such as: (1) any
fixes required to prevent the contributing factor in the future, (2) any
preparedness tasks that could help mitigate the problem if it came up
again, (3) remaining post-mortem steps, such as the internal email, as
well as the status-page public post, (4) any improvements to our
incident response process.*

<table>
<thead>
<tr class="header">
<th><strong>Action Item</strong></th>
<th><strong>Status</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Build monitoring and alert for message consumption and production backlog for puptoo</td>
<td><a href="https://projects.engineering.redhat.com/browse/RHCLOUD-5815"><span class="underline">Jira</span></a> - platform data</td>
</tr>
<tr class="even">
<td>(dajo) Avoid releasing services on Fridays?</td>
<td></td>
</tr>
<tr class="odd">
<td>Identify dashboard to be associated with “UploadElapsedTimesTooLongHard” alert</td>
<td>Follow up meeting - <a href="http://meet.google.com/umy-khnc-bcr"><span class="underline">4/21 @ 2pm EDT</span></a></td>
</tr>
<tr class="even">
<td>Update SOP and alert text to include link to dashboard above</td>
<td>AI from meeting ^</td>
</tr>
<tr class="odd">
<td><p>Draft best practice email to insights-dev:</p>
<p>Pin dependencies to specific versions</p>
<p>Specifically critical of kafka consumers, good practice for all dependent libraries</p>
<p>Include a link to steps for app teams to hook up to Payload Tracker (README is up to date)</p></td>
<td><p>Email - cmoore to draft and share for review</p>
<p><a href="https://docs.google.com/document/d/1wmCLH1O0cfKLjmUg5jyepFB6VPUUP3VJ9JGQ2Yv1098/edit?usp=sharing"><span class="underline">DRAFT</span></a></p></td>
</tr>
<tr class="even">
<td>Update alert to use prometheus labels to focus on core services and Advisor</td>
<td>Defer to below meeting</td>
</tr>
<tr class="odd">
<td>Kyle &amp; Richard to sync on how metrics are exposed</td>
<td>Kyle to schedule</td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
</tr>
</tbody>
</table>

## Messaging

### Internal

*The main Slack thread with most of the troubleshooting chatter can be
viewed
[here](https://ansible.slack.com/archives/CDVFMQU31/p1586211741092100).
This is where the all-clear was sent.*
