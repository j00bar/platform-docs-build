---
date: 2020-06-17 12:43:41.509703
title: 'Post Mortem Incident Report - Kafka issue:  Uploads processed 50+ times (4-6-2020)'
---
## <span dir="ltr">Overview</span>

<span dir="ltr">Uploads appeared to be processed very slowly and many
uploads were being processed 50 times in our apps. This caused a large
backlog of uploads to form.</span>

## <span dir="ltr">What Happened</span>

<span dir="ltr">We received several “insights-prod
(UploadElapsedTimesTooLongHard pagerduty mnm)” alerts during the weekend
of 4/4. On Monday, a group was formed to troubleshoot the issue which
surfaced that uploads were not being processed correctly and a large
backlog was forming.</span>

<span dir="ltr"></span>

<span dir="ltr">Between Monday and Tuesday, a group of engineers
embarked on several troubleshooting paths. The situation was initially
presented as a Kafka issue, which caused significant troubleshooting
effort to be focused on performance challenges.</span>

<span dir="ltr"></span>

<span dir="ltr">A change made on Thursday, April 2 to increase the Kafka
message size was identified as a potential complication. This change was
rolled back on the evening of April 6. On the morning of April 7, it was
determined that the message size change roll back did not resolve the
issue.</span>

<span dir="ltr"></span>

<span dir="ltr">Insights-puptoo was released on Friday that included a
newer version of the kafka-python library.</span>

<span dir="ltr"></span>

<span dir="ltr">Jjaggars-hypothesis:</span>

<span dir="ltr"></span>

<span dir="ltr">Kafka-python update means that insights-puptoo stopped
committing offsets. Eventually kafka would serve the partition again on
the next call to poll(). Downstream, every consumer got many many
duplicates generated by puptoo. The additional load caused some clients
to fail which triggered CG rebalances which meant that progress wasn’t
made downstream either.</span>

<span dir="ltr"></span>

<span dir="ltr"></span>

## <span dir="ltr">Contributing Factors</span>

*<span dir="ltr">Include a description of any conditions that
contributed to the issue. If there were any actions taken that
exacerbated the issue, also include them here with the intention of
learning from any mistakes made during the resolution process.</span>*

## <span dir="ltr">Resolution</span>

<span dir="ltr">*Puptoo was modified to use the* confluent\_kafka
*library.*</span>

## <span dir="ltr">Impact</span>

*<span dir="ltr">Be specific here. Include numbers such as customers
affected, cost to business, etc.</span>*

<span dir="ltr"></span>

<table>
<thead>
<tr class="header">
<th><span dir="ltr"></span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span dir="ltr">Upload processing throughput was severely degraded</span></td>
<td><span dir="ltr">4 days (Friday, April 3 - Tuesday, April 7)</span></td>
</tr>
</tbody>
</table>

<span dir="ltr"></span>

<span dir="ltr"></span>

<span dir="ltr"></span>

## <span dir="ltr">Timeline</span>

*<span dir="ltr">Some important times to include: (1) time the
contributing factor began, (2) time of the page, (3) time that the
status page was updated (i.e. when the incident became public), (4) time
of any significant actions, (5) time the SEV-2/1 ended, (6) links to
tools/logs that show how the timestamp was arrived at.</span>*

<span dir="ltr"></span>

<table>
<thead>
<tr class="header">
<th><strong><span dir="ltr">Time (EST)</span></strong></th>
<th><strong><span dir="ltr">Notes</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span dir="ltr">April 4, 5:58am</span></td>
<td><span dir="ltr">First <em>insights-prod (UploadElapsedTimesTooLongHard pagerduty mnm)</em> alert fires. It self-resolves in 10 minutes. This repeats several times over the course of the day</span></td>
</tr>
<tr class="even">
<td><span dir="ltr">April 4, 7:26pm</span></td>
<td><span dir="ltr">Cmoore silences the alert, plans to schedule call on Monday morning</span></td>
</tr>
<tr class="odd">
<td><span dir="ltr">April 6, 10am</span></td>
<td><span dir="ltr">Group forms on Hangout to discuss the weekend alerts. It is determined that there is a deeper issue, but the problem still presents as a Kafka performance issue. Group agrees to continue troubleshooting and meet at 3:30pm</span></td>
</tr>
<tr class="even">
<td><span dir="ltr">April 6, 3:30pm</span></td>
<td><span dir="ltr">Group reconvenes to continue troubleshooting. This leads to several avenues being explored over the next 2.5 hours.</span></td>
</tr>
<tr class="odd">
<td><span dir="ltr">April 6, 5:32pm</span></td>
<td><span dir="ltr">Decide to roll back Kafka message size change that was deployed on April 3. Reach out to shaund to understand impact to xjoin. Shaund pulls in chrisk.</span></td>
</tr>
<tr class="even">
<td><span dir="ltr">April 6, 6:01pm</span></td>
<td><span dir="ltr">Begin rolling back Kafka message size change</span></td>
</tr>
<tr class="odd">
<td><span dir="ltr">April 6, 6:20pm</span></td>
<td><span dir="ltr">Roll back complete, waiting for system to stabilize, agree to transition to Slack <a href="https://ansible.slack.com/archives/CDVFMQU31/p1586211741092100"><span class="underline">thread</span></a> and revisit later in the evening</span></td>
</tr>
<tr class="even">
<td><span dir="ltr">April 7, 1:34pm</span></td>
<td><span dir="ltr">Update puptoo to use confluent_kafka library</span></td>
</tr>
<tr class="odd">
<td><span dir="ltr">April 7, 3:00pm</span></td>
<td><span dir="ltr">sadams reports in Slack <a href="https://ansible.slack.com/archives/CDVFMQU31/p1586211741092100"><span class="underline">thread</span></a> that messages are being processed again</span></td>
</tr>
</tbody>
</table>

<span dir="ltr"></span>

<span dir="ltr"></span>

## <span dir="ltr">How’d We Do?</span>

### <span dir="ltr">What went well?</span>

*<span dir="ltr">List anything you did well and want to call out. It's
OK to not list anything.</span>*

  - > <span dir="ltr">Cross workstream collaboration worked well.
    > Several engineers across many teams gathered at 10am, broke up and
    > did additional research, gathered at 3:30 to reconvene</span>

  - > <span dir="ltr">Efficient communication was handled by the
    > troubleshooting team by Hangout and Slack</span>

### <span dir="ltr">What didn’t go so well?</span>

<span dir="ltr">*List anything you think we didn't do very well. The
intent is that we should follow up on all points here to improve our
processes.*</span>

  - > <span dir="ltr">The person on-call (Asa) was alarmed repeatedly,
    > but should have been empowered to mute the alarm until regular
    > working hours, since we didn’t escalate the issue over the
    > weekend.</span>

  - > <span dir="ltr">The alerting in place was very general, did not
    > provide enough detail to identify the issue at alert time</span>

  - > <span dir="ltr">Be more skeptical of systemic problems. Is the
    > problem observed the previous Friday indicative of a larger
    > problem?</span>

  - > <span dir="ltr">Alert was not specific enough to consistently be
    > actionable</span>
    
      - > <span dir="ltr">Previous occurrences have either self-resolved
        > or involved a specific action to fix</span>
    
      - > <span dir="ltr">Alert should be specific and actionable</span>

  - > <span dir="ltr"></span>

<span dir="ltr"></span>

## <span dir="ltr">Action Items</span>

<span dir="ltr">*Each action item should be in the form of a JIRA
ticket, and each ticket should have the same set of two tags:
“sev1\_YYYYMMDD” (such as sev1\_20150911) and simply “sev1”. Include
action items such as: (1) any fixes required to prevent the contributing
factor in the future, (2) any preparedness tasks that could help
mitigate the problem if it came up again, (3) remaining post-mortem
steps, such as the internal email, as well as the status-page public
post, (4) any improvements to our incident response process.*</span>

<span dir="ltr"></span>

<table>
<thead>
<tr class="header">
<th><strong><span dir="ltr">Action Item</span></strong></th>
<th><strong><span dir="ltr">Status</span></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span dir="ltr">Build monitoring and alert for message consumption and production backlog for puptoo</span></td>
<td><span dir="ltr"><a href="https://projects.engineering.redhat.com/browse/RHCLOUD-5815"><span class="underline">Jira</span></a> - platform data</span></td>
</tr>
<tr class="even">
<td><span dir="ltr">(dajo) Avoid releasing services on Fridays?</span></td>
<td><span dir="ltr"></span></td>
</tr>
<tr class="odd">
<td><span dir="ltr">Identify dashboard to be associated with “UploadElapsedTimesTooLongHard” alert</span></td>
<td><span dir="ltr">Follow up meeting - <a href="http://meet.google.com/umy-khnc-bcr"><span class="underline">4/21 @ 2pm EDT</span></a></span></td>
</tr>
<tr class="even">
<td><span dir="ltr">Update SOP and alert text to include link to dashboard above</span></td>
<td><span dir="ltr">AI from meeting ^</span></td>
</tr>
<tr class="odd">
<td><p><span dir="ltr">Draft best practice email to insights-dev:</span></p>
<p><span dir="ltr">Pin dependencies to specific versions</span></p>
<p><span dir="ltr">Specifically critical of kafka consumers, good practice for all dependent libraries</span></p>
<p><span dir="ltr">Include a link to steps for app teams to hook up to Payload Tracker (README is up to date)</span></p></td>
<td><p><span dir="ltr">Email - cmoore to draft and share for review</span></p>
<p><span dir="ltr"></span></p>
<p><span dir="ltr"><a href="https://docs.google.com/document/d/1wmCLH1O0cfKLjmUg5jyepFB6VPUUP3VJ9JGQ2Yv1098/edit?usp=sharing"><span class="underline">DRAFT</span></a></span></p></td>
</tr>
<tr class="even">
<td><span dir="ltr">Update alert to use prometheus labels to focus on core services and Advisor</span></td>
<td><p><span dir="ltr">Defer to below meeting</span> <span dir="ltr"></span></p>
<p><span dir="ltr"></span></p></td>
</tr>
<tr class="odd">
<td><span dir="ltr">Kyle &amp; Richard to sync on how metrics are exposed</span></td>
<td><span dir="ltr">Kyle to schedule</span></td>
</tr>
<tr class="even">
<td><span dir="ltr"></span></td>
<td><span dir="ltr"></span></td>
</tr>
<tr class="odd">
<td><span dir="ltr"></span></td>
<td><span dir="ltr"></span></td>
</tr>
<tr class="even">
<td><span dir="ltr"></span></td>
<td><span dir="ltr"></span></td>
</tr>
<tr class="odd">
<td><span dir="ltr"></span></td>
<td><span dir="ltr"></span></td>
</tr>
</tbody>
</table>

<span dir="ltr"></span>

## <span dir="ltr">Messaging</span>

### <span dir="ltr">Internal</span>

*<span dir="ltr">The main Slack thread with most of the troubleshooting
chatter can be viewed
[<span class="underline">here</span>](https://ansible.slack.com/archives/CDVFMQU31/p1586211741092100).
This is where the all-clear was sent.</span>*

<span dir="ltr"></span>
